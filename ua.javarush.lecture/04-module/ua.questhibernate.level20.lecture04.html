  BigData: Прийоми та стратегії розробки MapReduce-додатків <p>-------------------------------------- --</p> Map only job. Combine. Ланцюжки MapReduce-завдань. Distributed cache. Reduce Join. MapJoin. <p>----------------------------------------</p> <h2> 5.1 Map only job </h2> <p>Настав час описати різні прийоми, які дозволяють ефективно використовувати MapReduce для вирішення практичних завдань, а також показати деякі особливості Hadoop, які дозволяють спростити розробку або суттєво прискорити виконання MapReduce-завдання на кластері. </p> <p>Як ми пам'ятаємо, MapReduce складається зі стадій Map, Shuffle та Reduce. Як правило, у практичних завданнях найважчою виявляється стадія Shuffle, тому що на цій стадії відбувається сортування даних. Насправді існує низка завдань, у яких можна обійтися лише стадією Map. Ось приклади таких завдань: </p> <ul> <li>Фільтрування даних (наприклад, «Знайти всі записи з IP-адреси 123.123.123.123» у логах web-сервера); </li> <li>Перетворення даних («Видалити колонку в csv-логах»); </li> <li>Завантаження та вивантаження даних із зовнішнього джерела («Вставити всі записи з лога до бази даних»). </li> </ul> <p>Такі завдання вирішуються за допомогою Map-Only. При створенні Map-Only завдання в Hadoop потрібно вказати нульову кількість reducer'ів: </p> <img data-max-width="512" data-id="38d1e29c-fa4f-417d-ab05-44791ce19834" ://cdn.javarush.com/images/article/38d1e29c-fa4f-417d-ab05-44791ce19834/512.jpeg" alt=""> <p>Приклад конфігурації map-only завдання на hadoop: </p> <table > <tbody> <tr> <th>Native interface </th> <th>Hadoop Streaming Interface </th> </tr> <tr> <td><p>Вказати нульову кількість ред'юсерів при конфігурації job'a: < /p> <pre><code>job.setNumReduceTasks(0); </code></pre></td> <td><p>Не вказуємо ред'юсер і вказуємо нульову кількість ред'юсерів. Приклад: </p> <pre><code>hadoop jar hadoop-streaming.jar \-D mapred.reduce.tasks=0\ -input input_dir\ -output output_dir\ -mapper "python mapper.py"\ -file " mapper.py" </code></pre> </td> </tr> </tbody> </table> <p>Map Only jobs насправді можуть бути дуже корисними. Наприклад, у платформі Facetz.DCA виявлення характеристик користувачів з їхньої поведінки використовується саме один великий map-only, кожен маппер якого приймає на вхід користувача і вихід віддає його характеристики. </p> <h2>5.2 Combine </h2> <p>Як я вже писав, зазвичай найважча стадія при виконанні Map-Reduce завдання – це стадія shuffle. Відбувається це тому, що проміжні результати (вихід mapper'a) записуються на диск, сортуються та передаються по мережі. Проте існують завдання, у яких така поведінка здається не дуже розумною. Наприклад, у тій же задачі підрахунку слів у документах можна попередньо передагрегувати результати виходів кількох mapper'ів на одному вузлі map-reduce завдання, та передавати на reducer вже підсумовані значення кожної машини. </p> <img data-max-width="512" data-id="5d08e85a-d51f-43eb-8dad-32824da03ce9" src="https://cdn.javarush.com/images/article/5d08e85a-d51f -43eb-8dad-32824da03ce9/original.jpeg" alt=""> <p>У hadoop для цього можна визначити комбінуючу функцію, яка оброблятиме вихід частини mapper-ів. Комбінуюча функція дуже схожа на reduce – вона приймає на вхід вихід частини mapper'ів та видає агрегований результат для цих mapper'ів, тому дуже часто reducer використовують і як combiner. Важлива відмінність від reduce – <strong>на функцію, що комбінує, потрапляють не всі значення, що відповідають одному ключу</strong>. </p> <p>Більш того, hadoop не гарантує того, що комбінуюча функція взагалі буде виконана для виходу mapper'a. Тому комбінуюча функція не завжди застосовна, наприклад у разі пошуку медіанного значення по ключу. Тим не менш, у тих завданнях, де комбінуюча функція застосовна, її використання дозволяє досягти суттєвого приросту до швидкості виконання MapReduce-завдання. </p> <p>Використання Combiner'a на hadoop: </p> <table> <tbody> <tr> <th>Native Interface </th> <th>Hadoop streaming </th> </tr> < tr> <td><p>При конфігурації job-a вказати клас-Combiner. Як правило, він збігається з Reducer: </p> <pre><code>job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); </code></pre></td> <td><p>У параметрах командного рядка вказати команду -combiner. Як правило, ця команда збігається з командою reducer'a. Приклад: </p> <pre><code>hadoop jar hadoop-streaming.jar \-input input_dir\ -output output_dir\ -mapper "python mapper.py"\ -reducer "python reducer.py"\ -combiner "python reducer.py"\ -file "mapper.py"\ -file "reducer.py"\ </code></pre> </td> </tr> </tbody> </table> <h2>5.3 Ланцюжки MapReduce-завдань </h2> <p>Бують ситуації, коли для вирішення завдання одним MapReduce не обійтися. Наприклад, розглянемо трохи видозмінене завдання WordCount: є набір текстових документів, необхідно порахувати, скільки слів зустрілося від 1 до 1000 разів у наборі, скільки слів від 1001 до 2000, скільки від 2001 до 3000 і так далі. Для вирішення нам знадобиться 2 MapReduce job'а: </p> <ul> <li>Відозмінений wordcount, який для кожного слова розрахує, до якого з інтервалів воно потрапило; </li> <li>MapReduce, який підраховує, скільки разів у виході першого MapReduce зустрівся кожен з інтервалів. </li> </ul> <p>Рішення на псевдокоді: </p> <table> <tbody> <tr> <td><pre><code>#map1 map(doc): for word in doc: yield word, 1</code></pre></td> </d> ></pre></td> </tr> <tr> <td><pre><code>#map2 map(doc): interval, cnt = doc.split() yield interval, cnt </code> </pre></td> <td><pre><code>#reduce2 def reduce(interval, values): yield interval*1000, sum(values) </code></pre></td> </ tr> </tbody> </table> <p>Для того, щоб виконати послідовність MapReduce-задач на hadoop, досить просто як вхідні дані для другого завдання вказати папку, яка була вказана як output для першої і запустити їх по черзі . </p> <p>На практиці ланцюжки MapReduce-завдань можуть бути досить складними послідовностями, в яких MapReduce-завдання можуть бути підключені як послідовно, так і паралельно один одному. Для спрощення управління такими планами виконання завдань існують окремі інструменти типу oozie та luigi, яким буде присвячена окрема стаття цього циклу. </p> <img data-max-width="512" data-id="e75b9c38-86ca-4edc-a989-48ec6139d6ca" src="https://cdn.javarush.com/images/article/e75b9c38-86ca -4edc-a989-48ec6139d6ca/original.png" alt=""> <h2>5.4 Distributed cache </h2> <p>Важливим механізмом у Hadoop є Distributed Cache. Distributed Cache дозволяє додавати файли (наприклад, текстові файли, архіви, jar-файли) до оточення, в якому виконується MapReduce-завдання. </p> <p>Можно додавати файли, що зберігаються на HDFS, локальні файли (локальні для тієї машини, з якої виконується запуск завдання). Я вже неявно показував, як використовувати Distributed Cache разом із hadoop streaming: додаючи через опцію -file файли mapper.py та reducer.py. Насправді можна додавати не тільки mapper.py та reducer.py, а взагалі довільні файли, і потім користуватися ними начебто вони знаходяться у локальній папці. </p> <p>Використання Distributed Cache: </p> <table> <tody> <tr> <th>Native API </th> </tr> <tr> <td><pre class='language- java line-numbers'><code> //конфігурація Job'a JobConf job = new JobConf(); DistributedCache.addCacheFile(new URI("/myapp/lookup.dat#lookup.dat"), job); DistributedCache.addCacheArchive(new URI("/myapp/map.zip", job); DistributedCache.addFileToClassPath(new Path("/myapp/mylib.jar"), job); DistributedCache.addCacheArchive(new URI("/myapp/ mytar.tar", job); DistributedCache.addCacheArchive(new URI("/myapp/mytgz.tgz", job); // приклад використання в mapper-e: , V> { private Path [] localArchives; private Path [] localFiles; public void configure (JobConf job) { // отримуємо кешовані дані з архівів File f = new File (". zip.txt"); } public void map(K key, V-value, OutputCollector&lt;K, V> output, Reporter reporter) throws IOException { // використовуємо дані тут // ... // ... output.collect( k, v), } } </code></pre> </td> </tr> </tbod> </table> <table> <tbody> <tr> <th>Hadoop Streaming </th> < /tr> <tr> <td><p>#перелічуємо файли, які необхідно додати до distributed cache у параметрі –files.Параметр –files повинен йти перед іншими параметрами. </p> <pre><code>yarn hadoop-streaming.jar\ -files mapper.py,reducer.py,some_cached_data.txt\ -input '/some/input/path' \ -output '/some/output/ path' \ -mapper 'python mapper.py' \ -reducer 'python reducer.py' \ </code></pre> <p>приклад використання: </p> <pre><code>import sys #просто читаємо файл з локальної папки data = open('some_cached_data.txt').read() for line в sys.stdin() #processing input #use data here </code></pre> </tr> </tbody> < /table> <h2>5.5 Reduce Join </h2> <p>Ті, хто звик працювати з реляційними базами, часто користуються дуже зручною операцією Join, що дозволяє спільно обробити зміст деяких таблиць, об'єднавши їх за деяким ключем. Працюючи з великими даними таке завдання теж іноді виникає. Розглянемо наступний приклад: </p> <p>Є логи двох web-серверів, кожен лог має такий вигляд:</p> <pre><code>t\t </code></pre> <p>Приклад шматочка лога: </p> <pre><code>1446792139
178.78.82.1 /sphingosine/unhurrying.css 1446792139
126.31.163.222 /accentually.js 1446792139
154.164.149.83 /pyroacid/unkemptly.jpg 1446792139
202.27.13.181 /Chawia.js 1446792139 67.123.248.174 /morphographical/dismain.css 1446792139
226.74.123.135/phanerite.php 1446792139
157.109.106.104 /bisonant.css </code></pre> <p>Необхідно порахувати для кожної IP-адреси на якому з 2-х серверів він частіше заходив. Результат має бути поданий у вигляді: </p> <pre><code>\t </code></pre> <p>Приклад частини результату: </p> <pre><code>178.78.82.1 first 126.31. 163.222 second 154.164.149.83 second 226.74.123.135 first </code></pre> <p>На жаль, на відміну від реляційних баз даних, у загальному випадку об'єднання двох логів за ключом (у даному випадку – за IP-адресою) досить важку операцію і вирішується за допомогою 3-х MapReduce і патерну Reduce Join: </p> <img data-max-width="512" data-id="2398da09-c1ab-4233-a8ff-83f07aa50d55" src=" https://cdn.javarush.com/images/article/2398da09-c1ab-4233-a8ff-83f07aa50d55/original.png" alt=""> <p>ReduceJoin працює таким чином: </p> <p><strong >1)</strong> На кожен із вхідних логів запускається окремий MapReduce (Map only), що перетворює вхідні дані до наступного виду: </p> <pre><code>key -> (type, value </code></ pre> <p>Де key – це ключ, яким потрібно об'єднувати таблиці, Type – тип таблиці (first чи second у разі), а Value – це будь-які додаткові дані, прив'язані до ключу. </p> <p><strong>2)</strong> Виходи обох MapReduce подаються на вхід 3-му MapReduce, який, власне, і виконує об'єднання. Цей MapReduce містить порожній Mapper, який просто копіює вхідні дані. Далі shuffle розкладає дані за ключами та подає на вхід редьюсеру у вигляді: </p> <pre><code>key -> [(type, value)] </code></pre> <p>Важливо, що в цей момент на редьюсер потрапляють записи з обох логів і при цьому по полю типу можна ідентифікувати, з якого з двох логів потрапило конкретне значення. Значить даних достатньо, щоб вирішити вихідне завдання. У нашому випадку reducer просто повинен порахувати для кожного ключа записів, з яким тип зустрілося більше і вивести цей тип. </p> <h2>5.6 MapJoin </h2> <p>Паттерн ReduceJoin описує загальний випадок об'єднання двох логів за ключем. Однак є окремий випадок, при якому завдання можна суттєво спростити та прискорити. Це випадок, у якому одне із логів має розмір значно меншого розміру, ніж інший. Розглянемо таке завдання: </p> <p>Є 2 логи. Перший лог містить лог web-сервера (такий як у попередній задачі), другий файл (розміром в 100кб) містить відповідність URL-> Тематика. Приклад 2-го файлу: </p> <pre><code>/toyota.php auto /football/spartak.html sport /cars auto /finances/money business </code></pre> <p>Для кожного IP -Адреси необхідно розрахувати сторінки якої категорії з даної IP-адреси завантажувалися найчастіше. </p> <p>У цьому випадку нам теж необхідно виконати Join 2-х логів за URL. Однак у цьому випадку нам не обов'язково запускати 3 MapReduce, тому що другий лог повністю влізе в пам'ять. Для того, щоб вирішити задачу за допомогою 1-го MapReduce, ми можемо завантажити другий лог в Distributed Cache, а при ініціалізації Mapper'a просто рахувати його на згадку, поклавши його в словник -> topic. </p> <p>Далі завдання вирішується таким чином: </p> <p><strong>Map:</strong> </p> <pre><code># знаходимо тематику кожної зі сторінок першого лога</em> > input_line -> [ip, topic] </code></pre> <p><strong>Reduce:</strong> </p> <pre><code>Ip -> [topics] -> [ip, most_popular_topic] </code></pre> <p>Reduce отримує на вхід ip і список усіх тематик, просто обчислює, яка з тематик зустрілася найчастіше. Таким чином завдання вирішене за допомогою 1-го MapReduce, а власне Join взагалі відбувається всередині map (тому якби не потрібна була додаткова агрегація по ключу – можна було б обійтися MapOnly job-ом): </p><img data-max- width="256" data-id="c01e7545-8894-4036-86a2-28bbc2321949" src="https://cdn.javarush.com/images/article/c01e7545-8894-4036-84a2 " alt="">