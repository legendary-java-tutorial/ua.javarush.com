  Шардування: зворотній бік <p>----------------------------------------</p> > Як зашардити та втормозити в N разів? Про напівавтомат. Абсолютна ідеальна автоматика? Які бувають F()? Ціна, яку потрібно сплатити. Складний/довгий біль: решардінг. <p>----------------------------------------</p> <h2> 2.1 Як зашардити і втормозити в N разів? </h2> <p>Можна так зашардити і втормозити рівно N разів: </p> <ul> <li>Надіслати запити docs00...docs15 <strong>послідовно</strong>, а не паралельно. </li> <li>У простих запитах зробити вибірку <strong>не по ключу</strong>, WHERE something=234. </li> </ul> <p>У цьому випадку серіалізована частина (serial) займає не 1% і не 5%, а приблизно 20% у сучасних базах даних. Можна отримати і 50% серіалізованої частини, якщо звертатися до бази даних з дико ефективного бінарного протоколу або лінкувати її як динамічну бібліотеку скриптом на Python. </p> <p>Весь час обробки простого запиту займатимуть не розпаралелювані операції розбору запиту, підготовки плану тощо. Тобто гальмує не читання запису. </p> <p>Якщо ми розіб'ємо дані на 16 таблиць і запускатимемо послідовно, як прийнято в мові програмування PHP, наприклад, (він не дуже добре вміє запускати асинхронні процеси), то якраз і отримаємо уповільнення в 16 разів. А можливо навіть більше, тому що додадуться ще й network round-trips. </p> <p>Раптово, при шардуванні важливий вибір мови програмування. </p> <p>Пам'ятаємо про вибір мови програмування, тому що якщо надсилати запити до бази даних (або пошукового сервера) послідовно, то звідки взяти прискорення? Швидше, з'явиться уповільнення.</p> <h2>2.2 Про напівавтомат </h2> <p>Місцями вишукування інформаційних технологій вселяють хтонічний страх. Наприклад, MySQL з коробки у відсутності реалізації шардингу до певних версій точно, тим щонайменше розміри баз, експлуатованих у бою, доростають до непристойних величин. </p> <p>Марне людство в особі окремо взятих DBA мучиться роками і пише кілька поганих рішень для шардингу, побудованих незрозуміло на чому. Після цього пишеться одне більш-менш пристойне рішення для шардингу під назвою ProxySQL (MariaDB/Spider, PG/pg_shard/Citus...). Це добре відомий приклад цієї самої нашлепки. </p> <p>ProxySQL загалом, звичайно ж, повноцінне рішення enterprise-класу для open source, для роутингу та іншого. Але одне з розв'язуваних завдань — шардинг для бази даних, яка сама по собі шардити по-людськи не вміє. Розумієте, немає рубильника «shards=16», доводиться або у додатку переписувати кожен запит, які місцями багато, або між додатком і базою даних ставити якийсь проміжний шар, який дивиться: «Хм… SELECT*FROM documents? Так його треба розірвати на 16 маленьких SELECT * FROM server1.document1, SELECT * FROM server2.document2 - до цього сервера з таким логіном / паролем, до цього з іншим. Якщо один не відповів, то...» і т.д. Саме цим можуть займатися проміжні нашлепки. Вони є трохи меншими, ніж для всіх баз даних. Для PostgreSQL, наскільки я розумію, одночасно є і якісь вбудовані рішення (PostgresForeign Data Wrappers, на мою думку, вбудований у сам PostgreSQL), є зовнішні нашлепки. </p> <p>Конфігурування кожної конкретної нашлепки — це окрема гігантська тема, яка не поміститься в одну доповідь, тому ми обговоримо лише базові концепції. Давайте краще поговоримо трохи про теорію кайфу. </p> <h2>2.3 Абсолютна ідеальна автоматика? </h2> <p>Вся теорія кайфу у разі шардування в цій літері <strong>F()</strong>, базовий принцип завжди той самий грубо: shard_id = F(object). </p> <p>Шардування — це взагалі про що? У нас є 2 мільярди записів (або 64). Ми хочемо їх подрібнити на кілька шматків. Виникає несподіване питання – як? За яким принципом я свої 2 мільярди записів (або 64) повинен розкидати на доступні мені 16 серверів? </p> <p>Латентний математик у нас повинен підказати, що зрештою завжди є якась чарівна функція, яка за кожним документом (об'єктом, рядком тощо) визначить, у який шматок його покласти. </p> <p>Якщо заглибитися далі всередину математики, ця функція завжди залежить не тільки від самого об'єкта (самого рядка), але ще від зовнішніх налаштувань типу загальної кількості шардів. Функція, яка для кожного об'єкта повинна сказати, куди його класти, не може повертати значення більше, ніж є серверів системі. І функції трохи різні: </p> <pre><code>shard_func = F1(object); shard_id = F2(shard_func, ...); shard_id = F2(F1(object), current_num_shards, ...). </code></pre> <p>Але далі ми не закопуватимемося в ці нетрі окремих функцій, просто поговоримо які бувають чарівні функції F().</p> <h2>2.4 Які бувають F()? </h2> <p>Їх можна вигадати багато різних і багато різних механізмів реалізації. Приблизне коротке зведення: </p> <ul>F = rand() % nums_shards </li> <li>F = somehash(object.id) % num_shards </li> <li>F = object.date % num_shards </li> <li>F = object.user_id % num_shards </li> <li>... </li> <li>F = shard_table [ somehash() |… object.date |… ] </ li> </ul> <p>Цікавий факт — можна натурально розкидати всі дані випадково — черговий запис кидаємо на довільний сервер, довільне ядро, довільну таблицю. Щастя в цьому не буде особливого, але спрацює. </p> <p>Є трохи інтелектуальніші методи шардити за відтворюваною або навіть консистентною хеш-функцією, або шардити за якимсь атрибутом. Пройдемося за кожним методом. </p> <h4>F = rand() </h4> <p>Розкидати радом — не дуже правильний метод. Одна проблема: ми розкидали наші 2 млрд записів на тисячу серверів випадковим чином, і не знаємо, де запис лежить. Нам треба витягнути user_1, а де він не знаємо. Ідемо на тисячу серверів і перебираємо все якось це неефективно. </p> <h4>F = somehash() </h4> <p>Давайте розкидати користувачів по-дорослому: вважати відтворювану хеш-функцію від user_id, брати залишок від поділу на число серверів і звертатися одночасно до необхідного серверу. </p> <p>А навіщо ми це робимо? А потім, що у нас highload і в один сервер у нас більше нічого не влазить. Якби влазило, життя було б таке просте. </p> <p>Добре, ситуація вже покращилася, щоб отримати один запис, ми йдемо на один заздалегідь відомий сервер. Але якщо у нас є діапазон ключів, то в цьому діапазоні треба перебрати всі значення ключів і в межі сходити або на стільки шардів, скільки у нас ключів в діапазоні, або взагалі на кожен сервер. Ситуація, звичайно, покращала, але не для всіх запитів. Деякі запити постраждали. </p> <h4>Природне шардування (F = object.date % num_shards) </h4> <p>Іноді, тобто часто, 95% трафіку і 95% навантаження — це запити, які мають якесь природне шардування . Наприклад, 95% умовно соціально-аналітичних запитів зачіпає дані лише за останні 1 день, 3 дні, 7 днів, а 5%, що залишилися, звертаються до кількох останніх років. Але 95% запитів, таким чином, природно шардовані за датою, інтерес користувачів системи сфокусовано останніми кількома днями. </p> <p>У цьому випадку можна розділити дані за датою, наприклад, за одним днем, і за відповіддю на запит на звіт за якийсь день або об'єкт з цього дня на цей шард і йти. </p> <p>Життя покращується — ми тепер не тільки знаємо розташування конкретного об'єкта, а й про діапазон теж знаємо. Якщо у нас запитують не діапазон дат, а діапазон інших колонок, то, звичайно, доведеться перебирати усі шарди. Але за умовами гри у нас лише 5 % таких запитів. </p> <p>Начебто ми вигадали ідеальне рішення всього, але є дві проблеми: </p> <ul> <li>Це рішення заточено під конкретний кейс, коли 95% запитів задіяють лише останній тиждень. </li> <li>Оскільки 95% запитів чіпають останній тиждень, вони всі потраплятимуть на один шард, який цей останній тиждень обслуговує. Цей шард розплавиться, тоді як усі інші в цей час простоюватимуть. При цьому викидати їх не можна, архівні дані зберігати також потрібно. </li> </ul> <p>Не сказати, що це погана схема шардування — ми відрізали гарячі дані, проте з найбільш гарячим шардом треба щось робити. </p> <p>Проблема вирішується стисканнями, стрибками та припарками, тобто підвищенням кількості реплік для поточного дня, що горить, потім поступовим зниженням кількості реплік, коли цей день стає минулим і переходить до архіву. Тут немає ідеального рішення під назвою "треба просто чарівною хеш-функцією розмазати дані по кластеру не так". </p> <h2>2.5 Ціна, яку потрібно заплатити </h2> <p>Формально ми знаємо тепер знаємо «все». Щоправда, ми не знаємо одного гігантського головного болю і двох головних болів менше. </p> <h4>1. Простий біль: погано розмазало </h4> <p>Це приклад із підручника, який у бою майже не зустрічається, але раптом. </p> <ul> <li>Як приклад із датою, тільки без дати! </li> <li><strong>Ненавмисний</strong> нерівномірний (відчутно) розподіл. </li> </ul> <p>Вибрали механізм шардування, та/або дані змінилися, і, звичайно ж, PM не доніс вимоги (у нас же не буває помилок у коді, завжди PM вимоги не доносить), і розподіл став жахливо нерівномірним. Тобто промазали із критерієм. </p> <p>Щоб упіймати, треба дивитися розміри шардів. Проблему ми обов'язково побачимо в момент, коли у нас один шард або перегріється, або стане в 100 разів більше за інших. Полагодити можна просто заміною ключа або функції шардування. </p> <p>Це проста проблема, чесно кажучи, я не думаю, що хоча б одна людина зі ста нарветься на це в житті, але раптом хоч комусь допоможе. </p> <h4>2. «Непереможний» біль: агрегація, join </h4> </p> Як робити вибірки, які джойнять мільярд записів з однієї таблиці на мільярд записів з іншої таблиці? </p> <ul><li>Як «швидко» порахувати… WHERE randcol BETWEEN aaa AND bbb?</li> <li>Як «спритно» зробити… users_32shards JOIN posts_1024 shards? </li> </ul> <p>Коротка відповідь: ніяк, страждати! </p> <p>Якщо ви в першій таблиці розподілили мільярд записів на тисячу серверів, щоб вони швидше працювали, у другій таблиці зробили те саме, то природно тисяча на тисячу серверів повинні між собою говорити попарно. Мільйон зв'язків працювати добре не буде. Якщо ми робимо запити до бази (пошуку, сховища, document store або розподіленої файлової системи), які погано лягають на шардинг, ці запити дико гальмуватимуть. </p> <p>Важливий момент — <strong>якісь запити завжди невдало розмажуться і будуть гальмувати</strong>. Важливо спробувати мінімізувати їхній відсоток. Як наслідок, не треба робити гігантські джойни із мільярдом на мільярд записів. Якщо є можливість маленьку таблицю, щодо якої джойниш у гігантській розшарденній таблиці, реплікувати на всі вузли, треба зробити це. Якщо джойни насправді локальні якимось чином, наприклад, є можливість користувача та його пости розмістити поруч, зашардити їх однаково, і всі джойни робити в межах однієї машини треба робити саме так. </p> <p>Це окремий курс лекцій на три дні, тому переходимо до останнього пекельного болю та до різних алгоритмів боротьби з ним.</p> <h2>2.6. Складний/довгий біль: решардинг </h2> <p>Готуйтесь: якщо ви зашардили ваші дані перший раз у житті, то в середньому ще раз п'ять ви їх порішувати обов'язково. </p> <p>Скільки кластер не конфігуруй, все одно вирішувати. </p> <p>Якщо ви дуже розумні та щасливі, то перешардуйте, мінімум, один раз. Але один раз ви обов'язково, тому що в той момент, коли ви думаєте, що користувачеві достатньо 10 одиниць, хтось прямо зараз пише запит на 30, а в планах має запит на 100 одиниць невідомих ресурсів. Шардов завжди не вистачить. З першою схемою шардингу ви в будь-якому випадку промахнетеся - завжди доведеться або збільшувати кількість серверів докидати, або ще щось робити - загалом, якось дані переукладати. </p> <p>Добре, якщо у нас приємні ступені двійки: було 16 шардів-серверів, стало 32. Веселе, якщо було 17, стало 23 — два вазимно простих числа. Як же це роблять бази даних, можливо, вони мають якусь магію всередині? </p> <p>Правильна відповідь: ні, ніякої магії всередині немає, у них усередині є пекло.</p> <p>Далі розглянемо, що можна зробити «руками», може зрозуміємо «як автоматом». </p> <h4>У лоб #1. Переселити все </h4> <p>Для всіх об'єктів рахуємо NewF(object), перекладаємо на новий шард. </p> <p>Вірогідність збігу NewF()=OldF() невелика. </p> <p>Перекладемо майже все взагалі. </p> <p>Ой. </p> <p>Такого пекла, як перекласти всі 2 млрд записів зі старих шардів на нові, я сподіваюся, немає ніде. Наївний підхід зрозумілий: було 17 машин, додали 6 машин у кластер, перебрали 2 млрд записів, переклали їх із 17 машин на 23 машини. Раз на 10 років можна, напевно, навіть це зробити. Але загалом це поганий хід. </p> <h4>У лоб #2. Переселити половину </h4> <p>Наступне наївне поліпшення — давайте відмовимося від такої безглуздої схеми — заборонимо 17 машин вирішити в 23, і завжди вирішуватимемо 16 машин в 32 машини! Тоді нам з теорії доведеться перекласти рівно половину даних, і практично ми теж зможемо це зробити. </p> <p>Для всіх об'єктів вважаємо NewF(object), перекладаємо на новий шард. </p> <p>Було строго 2^N, стало строго 2^(N+1) шардів. </p> <p>Вірогідність збігу NewF()=OldF() дорівнює 0,5. </p> <p>Перекладемо приблизно 50% даних. </p> <p>Оптимально, але працює тільки для ступенів двійки. </p> <p>В принципі, все чудово, за винятком прив'язки до ступеня двійки за кількістю машин. Цей наївний підхід, хоч як це дивно, може спрацювати. </p> <p>Зверніть увагу, додаткове дроблення кластера за ступенями двійки в даному випадку ще й оптимальне. У будь-якому випадку, додаючи 16 машин до кластеру з 16, ми зобов'язані половину даних перекласти — рівно половину і перекладемо. </p> <p>Добре, але невже людство не винайшло нічого більше — постає запитання у допитливого розуму. </p> <h4>Веселіше #3. Consistent hashing </h4> <p>Звичайно, тут обов'язкова картинка з колом для consistent hashing.</p> <p>Якщо загуглити «consistent hashing», то обов'язково вилізе коло, вся видача колами заселена. </p> <p>Ідея: давайте ідентифікатори шардів (хеші) намалюємо на колі, а поверх відзначимо захешовані ідентифікатори серверів. Коли треба додати сервер, ставимо нову точку на коло, і те, що було близько до неї (і тільки те, що виявилося близько до неї), переселяємо. </p> <p>При додаванні шарду: переглядаємо не все, а лише 2 «сусіди», перекладаємо в середньому 1/n. </p> <p>При видаленні шарду: переглядаємо тільки шар, що видаляється, перекладаємо тільки його. Тип оптимуму. </p> <p>Дуже ефективно з точки зору мінімізації трафіку при додаванні шарду, і абсолютно огидно з точки зору нормального балансування даних. Тому що коли ми хешуємо всі ці об'єкти, які розподіляємо на велику кількість машин, ми це робимо відносно нерівномірно: точки по колу розставляться нерівномірно, і завантаження кожного конкретного вузла може відрізнятися від інших. </p> <p>Вирішувати цю проблему прийнято останнім рядком віртуальної ноди. Кожен вузол, кожен сервер на колі позначається однією точкою. Додаючи сервер, шард тощо, ми додаємо кілька точок. Щоразу, коли ми видаляємо щось, відповідно, видаляємо кілька точок і перекладаємо невелику частину даних. </p> <p>Я розповідаю про цей космос із колами, бо, наприклад, усередині Cassandra саме така схема. Тобто, коли вона у вас почала запис між нодами ганяти, знайте, що коло дивиться на вас і, напевно, не схвалює. </p> <p>Проте, в порівнянні з першими способами життя покращилося — ми вже переглядаємо при додаванні/видаленні шарду не всі записи, а лише частину, і перекладаємо тільки частину. </p> <p>Увага, питання: чи не можна покращити ще? І ще покращити і рівномірність завантаження шардів? - Кажуть, що можна! </p> <h4>Веселіше #4. Rendezvous/HRW </h4> <p>Наступна проста ідея (матеріал ж навчальний, тому нічого складного): shard_id = arg max hash(object_id, shard_id). </p> <p>Чому вона називається Rendezvous hashing, я не знаю, але знаю, чому вона називається Highest Random Weight. Її дуже просто візуалізувати так: </p> <p>У нас є, наприклад, 16 шардів. Для кожного об'єкта (рядки), який треба кудись покласти, обчислюємо 16 хешей, які залежать від об'єкта з номера шарда. У кого найвище значення хеш-функції, той переміг. </p> <p>Це так званий HRW-hashing, він же Rendezvous hashing. Тупа як палиця схема обчислення номера шарда, по-перше, на око простіше кіл і дає рівномірне завантаження, з іншого боку. </p> <p>Єдиний мінус у тому, що додавання нового шарда у нас погіршилося. Є ризик, що при додаванні нового шарда у нас все-таки якісь хеші зміняться і може виявитися необхідним переглянути все. Технологія видалення шарду не дуже змінилася. </p> <p>Ще одна проблема, це обчислювально важко при великій кількості шардів. </p> <h4>Веселіше #5. Ще техніки </h4> <p>Цікаво, що дослідження не стоять на місці, і Google щороку публікує якусь нову космічну техніку: </p> <ul> <li>Jump Hash — Google ‘2014. </li> <li>Multi Probe —Google ‘2015. </li> <li>Maglev - Google '2016. </li> </ul> <p>Якщо тематика вас зацікавила, можна прочитати багато дисертацій. Я наводжу ці дані для того, щоб було зрозуміло, що проблему не вирішено, супер-рішення, яке можна реалізувати у всіх базах немає. Досі люди захищають дисертацію. </p> <h3>Висновки </h3> <p>Є важлива базова техніка під назвою шардинг імені ще Галія Юлія Цезаря: «Поділяй і володарюй, володарюй і розділяй!». Якщо дані не влазять в один сервер, треба розбити їх на 20 серверів. </p> <p>Дізнавшись це все, має виникнути враження, що краще б не шардити. Якщо ви вирішили, що краще не шардити - це правильне відчуття. Якщо можна додати за 100 $ пам'яті в сервер і нічого не шардити, то так і треба зробити. При шардуванні з'явиться складна розподілена система з перекачуванням даних сюди-туди, укладанням даних невідомо куди. Якщо цього можна уникнути — треба цього уникнути. </p> <p>Краще це робити не руками, краще, щоб «база» (пошук, DFS, ...) сама вміла шардувати. У будь-якому випадку, рано чи пізно, високанавантаження настане і якось дані дробити доведеться. Не факт, що навіть якщо база вміє робити це сама, ви не нарветесь на якісь проблеми. Пам'ятайте про алгоритмічний фундаменталізм — треба розуміти, як усе влаштовано всередині. </p> <p>Налаштовуючи шардування перший раз акуратно вибирайте F(), думайте про запити, мережу, і т.п.